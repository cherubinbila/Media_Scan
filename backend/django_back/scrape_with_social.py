#!/usr/bin/env python3
"""
Script de scraping complet : Web + Facebook + Twitter
1. Scrape les articles du site web (RSS/HTML)
2. Classifie automatiquement les articles
3. Scrape les posts Facebook avec m√©triques d'engagement
4. Scrape les tweets Twitter avec m√©triques d'engagement
"""

import argparse
import os
from dotenv import load_dotenv
from database.db_manager import DatabaseManager
from scrapers.scraper_manager import ScraperManager
from scrapers.facebook_scraper import FacebookScraper
from scrapers.twitter_scraper import TwitterScraper

# Charger les variables d'environnement
load_dotenv()


def load_config_file(file_path: str) -> dict:
    """
    Charger un fichier de configuration
    
    Returns:
        Dictionnaire {url_site: identifiant}
    """
    config = {}
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if not line or line.startswith('#'):
                    continue
                
                parts = line.split('|')
                if len(parts) >= 3:
                    nom, url, identifier = parts[0], parts[1], parts[2]
                    config[url] = identifier
    
    except FileNotFoundError:
        print(f"‚ö†Ô∏è Fichier {file_path} non trouv√©")
    
    return config


def scrape_facebook_for_media(db: DatabaseManager, fb_scraper: FacebookScraper, 
                              media_id: int, fb_page: str, limit: int = 5):
    """Scraper Facebook pour un m√©dia"""
    print(f"\nüìò Scraping Facebook: {fb_page}")
    
    try:
        result = fb_scraper.scrape_page(fb_page, limit=limit)
        
        if result.get('error'):
            print(f"   ‚ùå Erreur: {result['error']}")
            return
        
        posts = result.get('posts', [])
        
        if not posts:
            print(f"   ‚ö†Ô∏è Aucun post r√©cup√©r√©")
            return
        
        # Sauvegarder les posts
        saved_count = 0
        for post in posts:
            try:
                db.add_facebook_post(
                    media_id=media_id,
                    post_id=post['post_id'],
                    message=post['message'],
                    url=post['url'],
                    image_url=post.get('image_url'),
                    date_publication=post['date_publication'],
                    likes=post['likes'],
                    comments=post['comments'],
                    shares=post['shares']
                )
                saved_count += 1
            except Exception:
                continue
        
        stats = result.get('stats', {})
        print(f"   ‚úÖ {saved_count} posts sauvegard√©s")
        print(f"   üìä Engagement: {stats.get('total_engagement', 0):,}")
    
    except Exception as e:
        print(f"   ‚ùå Erreur: {e}")


def scrape_twitter_for_media(db: DatabaseManager, tw_scraper: TwitterScraper,
                             media_id: int, tw_account: str, limit: int = 5):
    """Scraper Twitter pour un m√©dia"""
    print(f"\nüê¶ Scraping Twitter: @{tw_account}")
    
    try:
        result = tw_scraper.scrape_user(tw_account, max_results=limit)
        
        if result.get('error'):
            print(f"   ‚ùå Erreur: {result['error']}")
            return
        
        tweets = result.get('tweets', [])
        
        if not tweets:
            print(f"   ‚ö†Ô∏è Aucun tweet r√©cup√©r√©")
            return
        
        # Sauvegarder les tweets
        saved_count = 0
        for tweet in tweets:
            try:
                db.add_twitter_tweet(
                    media_id=media_id,
                    tweet_id=tweet['tweet_id'],
                    text=tweet['text'],
                    url=tweet['url'],
                    image_url=tweet.get('image_url'),
                    date_publication=tweet['date_publication'],
                    retweets=tweet['retweets'],
                    replies=tweet['replies'],
                    likes=tweet['likes'],
                    quotes=tweet['quotes'],
                    impressions=tweet['impressions']
                )
                saved_count += 1
            except Exception:
                continue
        
        stats = result.get('stats', {})
        print(f"   ‚úÖ {saved_count} tweets sauvegard√©s")
        print(f"   üìä Engagement: {stats.get('total_engagement', 0):,}")
    
    except Exception as e:
        print(f"   ‚ùå Erreur: {e}")


def main():
    parser = argparse.ArgumentParser(description='Scraping Web + Facebook + Twitter')
    parser.add_argument('--url', type=str, help='URL d\'un m√©dia sp√©cifique')
    parser.add_argument('--all', action='store_true', help='Scraper tous les sites')
    parser.add_argument('--days', type=int, default=30, help='Nombre de jours √† scraper')
    parser.add_argument('--fb-posts', type=int, default=5, 
                       help='Nombre de posts Facebook √† r√©cup√©rer')
    parser.add_argument('--tweets', type=int, default=5,
                       help='Nombre de tweets √† r√©cup√©rer')
    parser.add_argument('--skip-facebook', action='store_true',
                       help='Ignorer le scraping Facebook')
    parser.add_argument('--skip-twitter', action='store_true',
                       help='Ignorer le scraping Twitter')
    
    args = parser.parse_args()
    
    # Initialiser
    print("üîß Initialisation...")
    db = DatabaseManager()
    scraper_manager = ScraperManager(db, auto_classify=True)
    
    # Charger les configurations
    fb_pages = load_config_file('facebook_pages.txt')
    tw_accounts = load_config_file('twitter_accounts.txt')
    
    # Initialiser le scraper Facebook
    fb_scraper = None
    if not args.skip_facebook:
        fb_token = os.getenv('FACEBOOK_ACCESS_TOKEN')
        if fb_token:
            fb_scraper = FacebookScraper(fb_token)
            if fb_scraper.test_connection():
                print("‚úÖ Facebook API connect√©e")
            else:
                print("‚ö†Ô∏è Facebook API non accessible")
                fb_scraper = None
        else:
            print("‚ö†Ô∏è Token Facebook manquant")
    
    # Initialiser le scraper Twitter
    tw_scraper = None
    if not args.skip_twitter:
        tw_token = os.getenv('TWITTER_BEARER_TOKEN')
        if tw_token:
            tw_scraper = TwitterScraper(tw_token)
            if tw_scraper.test_connection():
                print("‚úÖ Twitter API connect√©e")
            else:
                print("‚ö†Ô∏è Twitter API non accessible")
                tw_scraper = None
        else:
            print("‚ö†Ô∏è Bearer Token Twitter manquant")
    
    print()
    
    # Scraper un site sp√©cifique
    if args.url:
        print("="*60)
        print(f"üéØ Scraping: {args.url}")
        print("="*60)
        
        # Scraping web
        count, method, message = scraper_manager.scrape_site(args.url, days=args.days)
        print(message)
        
        # R√©cup√©rer le m√©dia
        media = db.get_media_by_url(args.url)
        if media:
            # Scraping Facebook
            if fb_scraper and args.url in fb_pages:
                scrape_facebook_for_media(
                    db, fb_scraper, media.id, 
                    fb_pages[args.url], args.fb_posts
                )
            
            # Scraping Twitter
            if tw_scraper and args.url in tw_accounts:
                scrape_twitter_for_media(
                    db, tw_scraper, media.id,
                    tw_accounts[args.url], args.tweets
                )
    
    # Scraper tous les sites
    elif args.all:
        print("="*60)
        print("üöÄ SCRAPING MULTI-SITES")
        print("="*60)
        
        # Lire sites.txt
        try:
            with open('sites.txt', 'r', encoding='utf-8') as f:
                sites = [line.strip() for line in f 
                        if line.strip() and not line.startswith('#')]
        except FileNotFoundError:
            print("‚ùå Fichier sites.txt non trouv√©")
            return
        
        total_articles = 0
        
        for i, url in enumerate(sites, 1):
            print(f"\n[{i}/{len(sites)}] {url}")
            print("-"*60)
            
            # Scraping web
            count, method, message = scraper_manager.scrape_site(url, days=args.days)
            total_articles += count
            print(f"   {message}")
            
            # R√©cup√©rer le m√©dia
            media = db.get_media_by_url(url)
            if media:
                # Scraping Facebook
                if fb_scraper and url in fb_pages:
                    scrape_facebook_for_media(
                        db, fb_scraper, media.id,
                        fb_pages[url], args.fb_posts
                    )
                
                # Scraping Twitter
                if tw_scraper and url in tw_accounts:
                    scrape_twitter_for_media(
                        db, tw_scraper, media.id,
                        tw_accounts[url], args.tweets
                    )
        
        # R√©sum√©
        print("\n" + "="*60)
        print("üìä R√âSUM√â")
        print("="*60)
        print(f"‚úÖ Total articles: {total_articles}")
        
        # Afficher le classement
        print("\n" + "="*60)
        print("üèÜ CLASSEMENT DES M√âDIAS")
        print("="*60 + "\n")
        
        ranking = db.get_media_ranking_with_twitter(days=args.days)
        for i, media in enumerate(ranking[:5], 1):
            print(f"{i}. {media['nom']}")
            print(f"   Articles: {media['total_articles']}")
            if media['total_posts_facebook'] > 0:
                print(f"   Facebook: {media['engagement_total_fb']:,}")
            if media['total_tweets'] > 0:
                print(f"   Twitter: {media['engagement_total_tw']:,}")
            if media['engagement_total'] > 0:
                print(f"   Total: {media['engagement_total']:,}")
            print()
    
    else:
        print("‚ùå Sp√©cifiez --url ou --all")
        print("üí° Exemples:")
        print("   python scrape_with_social.py --url https://www.aib.media")
        print("   python scrape_with_social.py --all")
        print("   python scrape_with_social.py --all --skip-facebook")


if __name__ == '__main__':
    main()
