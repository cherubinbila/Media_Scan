#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script de scraping complet : Web + Facebook + Twitter
1. Scrape les articles du site web (RSS/HTML)
2. Classifie automatiquement les articles
3. Scrape les posts Facebook avec m√©triques d'engagement
4. Scrape les tweets Twitter avec m√©triques d'engagement
"""

import argparse
import os
import sys

# Forcer l'encodage UTF-8 pour Windows
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

from dotenv import load_dotenv
from database.db_manager import DatabaseManager
from scrapers.scraper_manager import ScraperManager
from scrapers.facebook_scraper import FacebookScraper
from scrapers.twitter_scraper import TwitterScraper

# Charger les variables d'environnement
load_dotenv()


def load_config_file(file_path: str) -> dict:
    """
    [DEPRECATED] Charger un fichier de configuration
    Conserv√© pour compatibilit√© mais non utilis√©
    
    Returns:
        Dictionnaire {url_site: identifiant}
    """
    return {}


def scrape_facebook_for_media(db: DatabaseManager, fb_scraper: FacebookScraper, 
                              media_id: int, fb_page: str, limit: int = 5):
    """Scraper Facebook pour un m√©dia"""
    print(f"\nüìò Scraping Facebook: {fb_page}")
    
    try:
        result = fb_scraper.scrape_page(fb_page, limit=limit)
        
        if result.get('error'):
            print(f"   ‚ùå Erreur: {result['error']}")
            return
        
        posts = result.get('posts', [])
        
        if not posts:
            print(f"   ‚ö†Ô∏è Aucun post r√©cup√©r√©")
            return
        
        # Sauvegarder les posts
        saved_count = 0
        for post in posts:
            try:
                db.add_facebook_post(
                    media_id=media_id,
                    post_id=post['post_id'],
                    message=post['message'],
                    url=post['url'],
                    image_url=post.get('image_url'),
                    date_publication=post['date_publication'],
                    likes=post['likes'],
                    comments=post['comments'],
                    shares=post['shares']
                )
                saved_count += 1
            except Exception:
                continue
        
        stats = result.get('stats', {})
        print(f"   ‚úÖ {saved_count} posts sauvegard√©s")
        print(f"   üìä Engagement: {stats.get('total_engagement', 0):,}")
    
    except Exception as e:
        print(f"   ‚ùå Erreur: {e}")


def scrape_twitter_for_media(db: DatabaseManager, tw_scraper: TwitterScraper,
                             media_id: int, tw_account: str, limit: int = 5):
    """Scraper Twitter pour un m√©dia"""
    print(f"\nüê¶ Scraping Twitter: @{tw_account}")
    
    try:
        result = tw_scraper.scrape_user(tw_account, max_results=limit)
        
        if result.get('error'):
            print(f"   ‚ùå Erreur: {result['error']}")
            return
        
        tweets = result.get('tweets', [])
        
        if not tweets:
            print(f"   ‚ö†Ô∏è Aucun tweet r√©cup√©r√©")
            return
        
        # Sauvegarder les tweets
        saved_count = 0
        for tweet in tweets:
            try:
                db.add_twitter_tweet(
                    media_id=media_id,
                    tweet_id=tweet['tweet_id'],
                    text=tweet['text'],
                    url=tweet['url'],
                    image_url=tweet.get('image_url'),
                    date_publication=tweet['date_publication'],
                    retweets=tweet['retweets'],
                    replies=tweet['replies'],
                    likes=tweet['likes'],
                    quotes=tweet['quotes'],
                    impressions=tweet['impressions']
                )
                saved_count += 1
            except Exception:
                continue
        
        stats = result.get('stats', {})
        print(f"   ‚úÖ {saved_count} tweets sauvegard√©s")
        print(f"   üìä Engagement: {stats.get('total_engagement', 0):,}")
    
    except Exception as e:
        print(f"   ‚ùå Erreur: {e}")


def main():
    parser = argparse.ArgumentParser(description='Scraping Web + Facebook + Twitter')
    parser.add_argument('--url', type=str, help='URL d\'un m√©dia sp√©cifique')
    parser.add_argument('--all', action='store_true', help='Scraper tous les sites')
    parser.add_argument('--days', type=int, default=30, help='Nombre de jours √† scraper')
    parser.add_argument('--fb-posts', type=int, default=5, 
                       help='Nombre de posts Facebook √† r√©cup√©rer')
    parser.add_argument('--tweets', type=int, default=5,
                       help='Nombre de tweets √† r√©cup√©rer')
    parser.add_argument('--skip-facebook', action='store_true',
                       help='Ignorer le scraping Facebook')
    parser.add_argument('--skip-twitter', action='store_true',
                       help='Ignorer le scraping Twitter')
    
    args = parser.parse_args()
    
    # Initialiser
    print("üîß Initialisation...")
    db = DatabaseManager()
    scraper_manager = ScraperManager(db, auto_classify=True)
    
    # Initialiser le scraper Facebook
    fb_scraper = None
    if not args.skip_facebook:
        fb_token = os.getenv('FACEBOOK_ACCESS_TOKEN')
        if fb_token:
            fb_scraper = FacebookScraper(fb_token)
            if fb_scraper.test_connection():
                print("‚úÖ Facebook API connect√©e")
            else:
                print("‚ö†Ô∏è Facebook API non accessible")
                fb_scraper = None
        else:
            print("‚ö†Ô∏è Token Facebook manquant")
    
    # Initialiser le scraper Twitter
    tw_scraper = None
    if not args.skip_twitter:
        tw_token = os.getenv('TWITTER_BEARER_TOKEN')
        if tw_token:
            tw_scraper = TwitterScraper(tw_token)
            if tw_scraper.test_connection():
                print("‚úÖ Twitter API connect√©e")
            else:
                print("‚ö†Ô∏è Twitter API non accessible")
                tw_scraper = None
        else:
            print("‚ö†Ô∏è Bearer Token Twitter manquant")
    
    print()
    
    # Scraper un site sp√©cifique
    if args.url:
        print("="*60)
        print(f"üéØ Scraping: {args.url}")
        print("="*60)
        
        # Scraping web
        count, method, message = scraper_manager.scrape_site(args.url, days=args.days)
        print(message)
        
        # R√©cup√©rer le m√©dia
        media = db.get_media_by_url(args.url)
        if media:
            # Scraping Facebook
            if fb_scraper and media.facebook_page:
                scrape_facebook_for_media(
                    db, fb_scraper, media.id, 
                    media.facebook_page, args.fb_posts
                )
            
            # Scraping Twitter
            if tw_scraper and media.twitter_account:
                scrape_twitter_for_media(
                    db, tw_scraper, media.id,
                    media.twitter_account, args.tweets
                )
    
    # Scraper tous les sites
    elif args.all:
        print("="*60)
        print("üöÄ SCRAPING MULTI-SITES (depuis table media)")
        print("="*60)
        
        # R√©cup√©rer tous les m√©dias actifs
        medias = db.get_all_medias(actif_only=True)
        
        if not medias:
            print("‚ùå Aucun m√©dia trouv√© dans la table media")
            return
        
        total_articles = 0
        
        for i, media in enumerate(medias, 1):
            print(f"\n[{i}/{len(medias)}] {media.nom} ({media.url})")
            print("-"*60)
            
            # Scraping web
            if media.url:
                count, method, message = scraper_manager.scrape_site(media.url, days=args.days)
                total_articles += count
                print(f"   {message}")
            
            # Scraping Facebook
            if fb_scraper and media.facebook_page:
                scrape_facebook_for_media(
                    db, fb_scraper, media.id,
                    media.facebook_page, args.fb_posts
                )
            
            # Scraping Twitter
            if tw_scraper and media.twitter_account:
                scrape_twitter_for_media(
                    db, tw_scraper, media.id,
                    media.twitter_account, args.tweets
                )
        
        # R√©sum√©
        print("\n" + "="*60)
        print("üìä R√âSUM√â")
        print("="*60)
        print(f"‚úÖ Total articles: {total_articles}")
        
        # Afficher le classement
        print("\n" + "="*60)
        print("üèÜ CLASSEMENT DES M√âDIAS")
        print("="*60 + "\n")
        
        ranking = db.get_media_ranking_with_twitter(days=args.days)
        for i, media in enumerate(ranking[:5], 1):
            print(f"{i}. {media['nom']}")
            print(f"   Articles: {media['total_articles']}")
            if media['total_posts_facebook'] > 0:
                print(f"   Facebook: {media['engagement_total_fb']:,}")
            if media['total_tweets'] > 0:
                print(f"   Twitter: {media['engagement_total_tw']:,}")
            if media['engagement_total'] > 0:
                print(f"   Total: {media['engagement_total']:,}")
            print()
        
        # Lancer la mod√©ration de contenu
        print("\n" + "="*60)
        print("üõ°Ô∏è MOD√âRATION DE CONTENU")
        print("="*60 + "\n")
        
        try:
            from analysis.content_moderator import ContentModerator
            
            moderator = ContentModerator()
            
            # V√©rifier la connexion √† Ollama
            if not moderator.check_ollama_status():
                print("‚ö†Ô∏è Ollama non disponible, mod√©ration ignor√©e")
            else:
                print("‚úÖ Ollama connect√©, lancement de la mod√©ration...\n")
                
                # Mod√©rer les articles
                articles = db.get_recent_articles(days=args.days, limit=1000)
                analyzed = 0
                flagged = 0
                
                for article in articles:
                    # V√©rifier si d√©j√† analys√©
                    existing = db.get_content_moderation('article', article.id)
                    if existing:
                        continue
                    
                    text = f"{article.titre}\n\n{article.contenu or article.extrait or ''}"
                    analysis = moderator.analyze_content(text, 'article')
                    db.add_content_moderation('article', article.id, analysis)
                    
                    analyzed += 1
                    if analysis['should_flag']:
                        flagged += 1
                        print(f"üö® Article {article.id} signal√© - {analysis['risk_level']} (Score: {analysis['risk_score']})")
                
                # Mod√©rer les posts Facebook
                if not args.skip_facebook:
                    posts = db.get_recent_facebook_posts(days=args.days, limit=500)
                    for post in posts:
                        existing = db.get_content_moderation('facebook_post', post.id)
                        if existing:
                            continue
                        
                        text = post.message or ""
                        if text.strip():
                            analysis = moderator.analyze_content(text, 'facebook_post')
                            db.add_content_moderation('facebook_post', post.id, analysis)
                            analyzed += 1
                            if analysis['should_flag']:
                                flagged += 1
                
                # Mod√©rer les tweets
                if not args.skip_twitter:
                    tweets = db.get_recent_twitter_tweets(days=args.days, limit=500)
                    for tweet in tweets:
                        existing = db.get_content_moderation('tweet', tweet.id)
                        if existing:
                            continue
                        
                        text = tweet.text or ""
                        if text.strip():
                            analysis = moderator.analyze_content(text, 'tweet')
                            db.add_content_moderation('tweet', tweet.id, analysis)
                            analyzed += 1
                            if analysis['should_flag']:
                                flagged += 1
                
                print(f"\n‚úÖ Mod√©ration termin√©e:")
                print(f"   Contenus analys√©s: {analyzed}")
                print(f"   Contenus signal√©s: {flagged}")
                if analyzed > 0:
                    print(f"   Taux de signalement: {(flagged/analyzed)*100:.1f}%")
        
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur lors de la mod√©ration: {e}")
            print("   Le scraping a r√©ussi mais la mod√©ration a √©chou√©")
    
    else:
        print("‚ùå Sp√©cifiez --url ou --all")
        print("üí° Exemples:")
        print("   python scrape_with_social.py --url https://www.aib.media")
        print("   python scrape_with_social.py --all")
        print("   python scrape_with_social.py --all --skip-facebook")


if __name__ == '__main__':
    main()
