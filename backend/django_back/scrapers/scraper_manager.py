"""
Gestionnaire principal de scraping avec fallback automatique
PrioritÃ©: RSS > HTML Scraping
Classification automatique aprÃ¨s scraping
"""

from typing import List, Tuple
from urllib.parse import urlparse

from database.db_manager import DatabaseManager
from database.models import Article
from .rss_scraper import RSScraper
from .smart_html_scraper import SmartHTMLScraper
from analysis.theme_classifier import ThemeClassifier


class ScraperManager:
    """Gestionnaire de scraping intelligent avec RSS et HTML"""
    
    def __init__(self, db_manager: DatabaseManager, auto_classify: bool = True):
        """
        Initialise le gestionnaire
        
        Args:
            db_manager: Instance de DatabaseManager
            auto_classify: Activer la classification automatique aprÃ¨s scraping
        """
        self.db = db_manager
        self.auto_classify = auto_classify
        self.classifier = None
        
        # Initialiser le classificateur si activÃ©
        if self.auto_classify:
            try:
                self.classifier = ThemeClassifier()
                # VÃ©rifier si Ollama est accessible
                if not self.classifier.check_ollama_status():
                    print("âš ï¸ Ollama non accessible, classification dÃ©sactivÃ©e")
                    self.auto_classify = False
            except Exception as e:
                print(f"âš ï¸ Erreur initialisation classificateur: {e}")
                self.auto_classify = False
    
    def scrape_site(self, url: str, days: int = 30) -> Tuple[int, str, str]:
        """
        Scraper un site avec RSS en prioritÃ©, sinon HTML
        
        Args:
            url: URL du site Ã  scraper
            days: Nombre de jours Ã  rÃ©cupÃ©rer
        
        Returns:
            Tuple (nombre d'articles, mÃ©thode utilisÃ©e, message)
        """
        # Nettoyer l'URL
        url = url.strip().rstrip('/')
        
        # Extraire le nom du domaine pour le nom du mÃ©dia
        domain = urlparse(url).netloc
        media_name = domain.replace('www.', '').split('.')[0].capitalize()
        
        print(f"\n{'='*60}")
        print(f"ğŸ¯ Scraping: {media_name} ({url})")
        print(f"{'='*60}\n")
        
        try:
            # Essayer d'abord avec RSS
            print(f"ğŸ”„ Tentative 1/2: Scraping RSS...")
            rss_scraper = RSScraper(url)
            articles = rss_scraper.scrape(media_id=0, days=days)  # media_id temporaire
            
            # Si RSS a fonctionnÃ©
            if len(articles) > 0:
                # Ajouter ou rÃ©cupÃ©rer le mÃ©dia
                media_id = self.db.add_media(media_name, url)
                
                # Mettre Ã  jour les media_id
                for article in articles:
                    article.media_id = media_id
                
                # Sauvegarder
                saved_count, new_article_ids = self._save_articles(articles)
                
                # Classification automatique des nouveaux articles
                if self.auto_classify and new_article_ids:
                    self._classify_articles(new_article_ids)
                
                # Mettre Ã  jour la date de derniÃ¨re collecte
                self.db.update_media_last_scrape(media_id)
                
                # Logger
                self.db.add_scraping_log(
                    media_id=media_id,
                    status='success',
                    methode='rss_feed',
                    articles_collectes=saved_count,
                    message=f"{saved_count} articles collectÃ©s via RSS"
                )
                
                return saved_count, 'rss_feed', f"âœ… {saved_count} articles collectÃ©s via RSS"
            
            # Si RSS n'a pas fonctionnÃ©, fallback vers HTML
            print(f"\nğŸ”„ Tentative 2/2: Scraping HTML...")
            scraper = SmartHTMLScraper(url)
            
            # Ajouter/mettre Ã  jour le mÃ©dia
            media_id = self.db.add_media(media_name, url, 'html')
            
            # Scraper les articles
            articles = scraper.scrape(media_id, days=days, max_articles=100)
            
            # Sauvegarder en base
            saved_count, new_article_ids = self._save_articles(articles)
            
            # Classification automatique des nouveaux articles
            if self.auto_classify and new_article_ids:
                self._classify_articles(new_article_ids)
            
            # Mettre Ã  jour la date de derniÃ¨re collecte
            self.db.update_media_last_scrape(media_id)
            
            # Logger
            self.db.add_scraping_log(
                media_id=media_id,
                status='success' if saved_count > 0 else 'partial',
                methode='html_scraping',
                articles_collectes=saved_count,
                message=f"{saved_count} articles collectÃ©s via scraping HTML"
            )
            
            return saved_count, 'html_scraping', f"âœ… {saved_count} articles collectÃ©s via scraping HTML"
        
        except Exception as e:
            error_msg = f"âŒ Erreur scraping: {e}"
            print(error_msg)
            
            # Logger l'erreur
            media_id = self.db.add_media(media_name, url, 'unknown')
            self.db.add_scraping_log(
                media_id=media_id,
                status='error',
                methode='html_scraping',
                articles_collectes=0,
                message=str(e)
            )
            
            return 0, 'error', error_msg
    
    def _save_articles(self, articles: List[Article]) -> Tuple[int, List[int]]:
        """
        Sauvegarder les articles en base de donnÃ©es
        
        Args:
            articles: Liste d'articles Ã  sauvegarder
        
        Returns:
            Tuple (nombre d'articles sauvegardÃ©s, liste des IDs des nouveaux articles)
        """
        saved_count = 0
        duplicate_count = 0
        new_article_ids = []
        
        for article in articles:
            # VÃ©rifier si l'article existe dÃ©jÃ 
            if not self.db.article_exists(article.url):
                article_id = self.db.add_article(article)
                if article_id:
                    saved_count += 1
                    new_article_ids.append(article_id)
            else:
                duplicate_count += 1
        
        if duplicate_count > 0:
            print(f"   ğŸ’¾ {saved_count} nouveaux articles, {duplicate_count} doublons ignorÃ©s")
        
        return saved_count, new_article_ids
    
    def _classify_articles(self, article_ids: List[int]):
        """
        Classifier automatiquement les articles
        
        Args:
            article_ids: Liste des IDs d'articles Ã  classifier
        """
        if not self.classifier or not article_ids:
            return
        
        print(f"\nğŸ¤– Classification automatique de {len(article_ids)} articles...")
        
        classified_count = 0
        errors = 0
        
        for article_id in article_ids:
            try:
                # RÃ©cupÃ©rer l'article
                article = self.db.get_article(article_id)
                if not article:
                    continue
                
                # Classifier
                result = self.classifier.classify_article(
                    article.get('titre', ''),
                    article.get('contenu', '')
                )
                
                # Sauvegarder la classification
                self.db.add_classification(
                    article_id=article_id,
                    categorie=result['categorie'],
                    confiance=result['confiance'],
                    mots_cles=result.get('mots_cles', []),
                    justification=result.get('justification', ''),
                    methode=result.get('methode', 'mistral_ollama')
                )
                
                classified_count += 1
            
            except Exception as e:
                errors += 1
                continue
        
        if classified_count > 0:
            print(f"   âœ… {classified_count} articles classifiÃ©s")
        if errors > 0:
            print(f"   âš ï¸ {errors} erreurs")
    
    def scrape_all_sites(self, sites_file: str = 'sites.txt', days: int = 30) -> dict:
        """
        Scraper tous les sites listÃ©s dans un fichier
        
        Args:
            sites_file: Chemin vers le fichier contenant les URLs
            days: Nombre de jours Ã  rÃ©cupÃ©rer
        
        Returns:
            Dictionnaire avec les statistiques
        """
        print("\n" + "="*60)
        print("ğŸš€ MÃ‰DIA-SCAN - Collecte Multi-Sites")
        print("="*60)
        
        # Lire le fichier des sites
        try:
            with open(sites_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
        except FileNotFoundError:
            print(f"âŒ Fichier {sites_file} non trouvÃ©")
            return {}
        
        # Filtrer les lignes (ignorer commentaires et lignes vides)
        urls = []
        for line in lines:
            line = line.strip()
            if line and not line.startswith('#'):
                urls.append(line)
        
        print(f"\nğŸ“‹ {len(urls)} sites Ã  scraper")
        print(f"ğŸ“… PÃ©riode: {days} derniers jours\n")
        
        # Statistiques
        stats = {
            'total_sites': len(urls),
            'success': 0,
            'errors': 0,
            'total_articles': 0,
            'by_method': {
                'html_scraping': 0,
                'error': 0
            },
            'details': []
        }
        
        # Scraper chaque site
        for i, url in enumerate(urls, 1):
            print(f"\n[{i}/{len(urls)}] Traitement de {url}...")
            
            count, method, message = self.scrape_site(url, days=days)
            
            if count > 0:
                stats['success'] += 1
                stats['total_articles'] += count
            else:
                stats['errors'] += 1
            
            stats['by_method'][method] = stats['by_method'].get(method, 0) + count
            
            stats['details'].append({
                'url': url,
                'articles': count,
                'method': method,
                'message': message
            })
            
            print(message)
        
        # Afficher le rÃ©sumÃ©
        self._print_summary(stats)
        
        return stats
    
    def _print_summary(self, stats: dict):
        """Afficher le rÃ©sumÃ© de la collecte"""
        print("\n" + "="*60)
        print("ğŸ“Š RÃ‰SUMÃ‰ DE LA COLLECTE")
        print("="*60)
        print(f"\nâœ… Sites traitÃ©s: {stats['total_sites']}")
        print(f"   â€¢ SuccÃ¨s: {stats['success']}")
        print(f"   â€¢ Erreurs: {stats['errors']}")
        print(f"\nğŸ“° Total articles collectÃ©s: {stats['total_articles']}")
        print(f"\nğŸ”§ Par mÃ©thode:")
        print(f"   â€¢ HTML Scraping: {stats['by_method'].get('html_scraping', 0)} articles")
        
        print(f"\nğŸ“‹ DÃ©tails par site:")
        for detail in stats['details']:
            status = "âœ…" if detail['articles'] > 0 else "âŒ"
            print(f"   {status} {detail['url']}: {detail['articles']} articles ({detail['method']})")
        
        print("\n" + "="*60)
