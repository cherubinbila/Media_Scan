"""
Scheduler pour l'automatisation du scraping
"""

import threading
import time
import subprocess
import sys
from datetime import datetime
from pathlib import Path

from database.db_manager import DatabaseManager


class ScrapingScheduler:
    """Scheduler pour exÃ©cuter automatiquement les tÃ¢ches de scraping"""
    
    def __init__(self, check_interval: int = 60):
        """
        Initialise le scheduler
        
        Args:
            check_interval: Intervalle de vÃ©rification en secondes (dÃ©faut: 60s)
        """
        self.check_interval = check_interval
        self.db = DatabaseManager()
        self.running = False
        self.thread = None
    
    def start(self):
        """DÃ©marre le scheduler en arriÃ¨re-plan"""
        if self.running:
            print("âš ï¸  Le scheduler est dÃ©jÃ  en cours d'exÃ©cution")
            return
        
        self.running = True
        self.thread = threading.Thread(target=self._run, daemon=True)
        self.thread.start()
        print(f"âœ… Scheduler dÃ©marrÃ© (vÃ©rification toutes les {self.check_interval}s)")
    
    def stop(self):
        """ArrÃªte le scheduler"""
        self.running = False
        if self.thread:
            self.thread.join(timeout=5)
        print("ðŸ›‘ Scheduler arrÃªtÃ©")
    
    def _run(self):
        """Boucle principale du scheduler"""
        while self.running:
            try:
                self._check_and_execute()
            except Exception as e:
                print(f"âŒ Erreur dans le scheduler: {e}")
            
            # Attendre avant la prochaine vÃ©rification
            time.sleep(self.check_interval)
    
    def _check_and_execute(self):
        """VÃ©rifie si une tÃ¢che doit Ãªtre exÃ©cutÃ©e"""
        schedule = self.db.get_scraping_schedule()
        
        if not schedule or not schedule['enabled']:
            return
        
        # VÃ©rifier si next_run est dÃ©passÃ©
        next_run = datetime.fromisoformat(schedule['next_run'])
        now = datetime.now()
        
        if now >= next_run:
            print(f"ðŸš€ Lancement du scraping automatique (frÃ©quence: {schedule['frequency']})")
            self._execute_scraping(schedule)
    
    def _execute_scraping(self, schedule: dict):
        """ExÃ©cute le scraping automatique"""
        try:
            # CrÃ©er une tÃ¢che de scraping
            task_id = self.db.create_scraping_task('automatic', {
                'frequency': schedule['frequency'],
                'days': schedule['days'],
                'fb_posts': schedule['fb_posts'],
                'tweets': schedule['tweets']
            })
            
            # Construire la commande
            script_path = Path(__file__).parent.parent / 'scrape_with_social.py'
            cmd = [
                sys.executable,
                str(script_path),
                '--all',
                '--days', str(schedule['days']),
                '--fb-posts', str(schedule['fb_posts']),
                '--tweets', str(schedule['tweets'])
            ]
            
            # ExÃ©cuter le scraping
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=600  # 10 minutes max
            )
            
            if result.returncode == 0:
                # Compter les rÃ©sultats
                days = schedule['days']
                articles = self.db.get_recent_articles(days=days, limit=10000)
                fb_posts = self.db.get_recent_facebook_posts(days=days, limit=10000)
                tweets = self.db.get_recent_twitter_tweets(days=days, limit=10000)
                
                total_articles = len(articles)
                total_fb_posts = len(fb_posts)
                total_tweets = len(tweets)
                
                # Mettre Ã  jour la tÃ¢che
                self.db.update_scraping_task(
                    task_id, 'completed',
                    total_articles=total_articles,
                    total_fb_posts=total_fb_posts,
                    total_tweets=total_tweets
                )
                
                print(f"âœ… Scraping automatique terminÃ©: {total_articles} articles, {total_fb_posts} posts FB, {total_tweets} tweets")
            else:
                error_msg = result.stderr or 'Erreur inconnue'
                self.db.update_scraping_task(task_id, 'failed', error_message=error_msg)
                print(f"âŒ Ã‰chec du scraping automatique: {error_msg}")
            
            # Mettre Ã  jour last_run et next_run
            self.db.update_schedule_last_run()
            
        except subprocess.TimeoutExpired:
            self.db.update_scraping_task(task_id, 'failed', error_message='Timeout')
            print("âŒ Le scraping automatique a pris trop de temps")
        except Exception as e:
            if 'task_id' in locals():
                self.db.update_scraping_task(task_id, 'failed', error_message=str(e))
            print(f"âŒ Erreur lors du scraping automatique: {e}")


# Instance globale du scheduler
_scheduler_instance = None


def get_scheduler() -> ScrapingScheduler:
    """RÃ©cupÃ¨re l'instance globale du scheduler"""
    global _scheduler_instance
    if _scheduler_instance is None:
        _scheduler_instance = ScrapingScheduler()
    return _scheduler_instance


def start_scheduler():
    """DÃ©marre le scheduler global"""
    scheduler = get_scheduler()
    scheduler.start()


def stop_scheduler():
    """ArrÃªte le scheduler global"""
    scheduler = get_scheduler()
    scheduler.stop()
